---
title: "Musical Sophistication and Valence Estimation"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    theme: cosmo
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(plotly)
library(Cairo)
library(DT)
library(reshape2)
library(data.table)
library(varhandle)
library(readxl)
library(dplyr)
```


```{r survey data}
#data cleaning 

data_wide <-
  read_csv('survey upd1.csv', TRUE)

#deleting irrelevant columns
data_wide <- data_wide[, -c(1:8, 24, 28:29)]

#COLUMNS 1:18 ARE MSI RESULTS
#COLUMN 19 IS SONG ORDER

#deleting all instances of 'MSI' in front of a value
data_wide <- data.frame(lapply(data_wide, function(x){
  gsub("MSI", "", x)
}))

#renaming the MSI columns
names(data_wide)[1:18] = paste0("MSI", 1:18)

#renaming the valence columns
names(data_wide)[c(20:69)[seq(1, 50, 2)]] = paste0("song", 1:25, "_valence")

#renaming the familiarity columns
names(data_wide)[c(20:69)[seq(2, 50, 2)]] = paste0("song", 1:25, "_familiarity")

#splitting the columns from "songID_valenceEst" to column "songID" and column "valenceEst"
setDT(data_wide)
cols <- grep("song", names(data_wide), value = TRUE)
for (i in cols) {
  temp <- tstrsplit(data_wide[[i]], "_")
  set(data_wide, j = sprintf("%s_%d", i, seq_along(temp)), value = temp)
  set(data_wide, j = i, value = NULL)
}

#deleting duplicate-value columns (familiarity_1)
data_wide <- data_wide[, -c(20:119)[seq(3, 100, 4)]]

names(data_wide) <- gsub("valence_1", "ID", names(data_wide))
names(data_wide) <- gsub("valence_2", "estVal", names(data_wide))
names(data_wide) <- gsub("familiarity_2", "familiar", names(data_wide))

data_wide <- unfactor(data_wide)

part_data_wide <-
  read_csv2("participants_results.csv")

#deleting irrelevant columns
part_data_wide <- part_data_wide[, -1]

#renaming the deviation columns
names(part_data_wide)[c(4:6)] = paste0("dev", 1:3)
names(part_data_wide)[1] = "MSI_total"
names(part_data_wide)[2] = "ValOff_total"

data_wide$MSI1 <- part_data_wide$MSI_total
data_wide <- data_wide[, -c(2:18)]
names(data_wide)[1] = "MSI_TOTAL"

```



```{r lengthening}

data_long <- melt(setDT(data_wide), id=1L,
                  measure = patterns("ID$", "estVal$", "familiar$"),
                  value.name = c("song_ID", "estVal", "familiarity"))

data_long$song_ID <- as.numeric(as.character(data_long$song_ID))
data_long$estVal <- as.numeric(as.character(data_long$estVal))
data_long$variable <- as.numeric(as.character(data_long$variable))

data_long$genre <-
  ifelse(data_long$song_ID < 21, "Pop",
         ifelse(data_long$song_ID < 41, "Rock",
                ifelse(data_long$song_ID < 61, "Dance",
                       ifelse(data_long$song_ID < 81, "Electronic",
                              ifelse(data_long$song_ID < 101, "House",
                                     ifelse(data_long$song_ID < 121, "Soundtracks",
                                            ifelse(data_long$song_ID < 141, "Hip-Hop",
                                                   ifelse(data_long$song_ID < 161, "Singer/Songwriter",
                                                          ifelse(data_long$song_ID < 181, "Classical",
                                                                 ifelse(data_long$song_ID < 201, "R&B",
                                                                        ifelse(data_long$song_ID < 221, "Soul/Blues", "Metal")
                                                                        )
                                                                 )
                                                          )
                                                   )
                                            )
                                     )
                              )
                       )
                )
         )
  
data_long <- data_long[order(data_long$song_ID)]


```

```{r valencing}
valences_data <-
  read_csv("valences.csv")

for(i in 1:nrow(data_long)) {
    data_long$truVal <- valences_data$valence[data_long$song_ID]
}

data_long$signValDev <- data_long$estVal - data_long$truVal

```


Main Findings {data-orientation=rows}
===

Sidebar {.sidebar data-width=350}
---

[**Study introduction**](#background)

**Study abstract**

The present study investigated the relationship between musical sophistication and musical valence estimation. This was done by having participants listen to a random selection of songs, representative of 12 genres, and collecting their valence estimations of each song via an [R Shiny app](http://rezaenay.shinyapps.io/everydaymusiclistening/). Musical sophistication was measured using the Gold-MSI questionnaire, and musical valence estimation was measured using a 7-point Likert scale. The results showed no negative correlation between the two measures. Based on the above, we conclude that there is no relationship between musical sophistication and musical valence estimation.

**Explanation of terms**

**Valence** - the intrinsic positiveness that is associated, in this case, with a musical stimulus. High valences indicates positive emotional states such as happiness/cheerfulness, whereas low valence indicates negative emotional dimensions such as sadness/gloominess.

**Musical sophistication** - the musical attitudes, skills and expertise of an individual.

**Hypothesis**

There is a negative relationship between musical sophistication and musical valence estimation error.

**Analysis & Results**

Analysis was conducted using R's built-in cor.test() function, running a one-sided bivariate correlation.

Correlation: Gold-MSI & Valence Estimation Error 

<font style = "font-family: times, serif; font-size:16pt;"> (*r* = 0.13, *p* = 0.79) </font>

**Scatterplot interpretation**

Higher scores on the x-axis (Gold-MSI Score) indicate higher musical sophistication.

Higher scores on the y-axis (Average Valence Estimation Error) indicate, on average, higher error (and in turn, lower accuracy) when estimating a musical fragment's valence.

Column
---

### Scatterplot: Musical Sophistication and Valence Estimation Error {data-height=800}

```{r mainplot, fig.width = 8}

options(repr.plot.width=800, repr.plot.height=500)
mainplot <- part_data_wide %>%
  ggplot(aes(x = MSI_total, y = ValOff_total/25)) +
  geom_point() +
  geom_smooth(method = "lm") +
  annotate(x = 100,
           y = 37/25,
           label=paste("r = ",
                       round(cor(part_data_wide$MSI_total, part_data_wide$ValOff_total),2)
                       ),
         geom="text", size=5) +
  scale_x_continuous(name = 'Gold-MSI Score') +
  scale_y_continuous(name = 'Average Estimated Valence Error') +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "gray")
        )

ggplotly(mainplot)
```


Column {.tabset}
---

### Interactive Experiment: Intro

**We challenge you to beat our participants!**

Grab a pen and paper! In this experiment demo you get to experience what our research is about in an interactive way. In the following tabs we have included short versions of the different parts in our survey. 

First, you get to make a mini Gold-MSI test. The result places you on the musical sophistication scale. The higher your score, the more knowledge and experience you have with music.

The second part consists of a few songs that we used in the survey. It is up to you to determine the valence value of each song to the best of your ability! For both the Gold-MSI test and the song estimation test, the results are on the subsequent tab.

How to grade yourself is explained on the answer tabs. After you have determined your scores, you can see in the graph where you would have approximately been in our sample and if you have surpassed the participants. Good luck!

### Mini-MSI

I spend a lot of my free time doing music-related activities.

1 --- 2 --- 3 --- 4 --- 5 --- 6 --- 7 

I can compare and discuss differences between two performances or versions of the same piece of music.

1 --- 2 --- 3 --- 4 --- 5 --- 6 --- 7 

When I sing, I have no idea whether I'm in tune or not.

1 --- 2 --- 3 --- 4 --- 5 --- 6 --- 7 

At the peak of my interest, I practiced ___ hours per day on my primary instrument.

0 --- 0.5 --- 1 --- 1.5 --- 2 --- 3-4 --- 5 or more


### Musical Fragments

What do you think is the valence of these four musical fragements? These values range from 1 to 7, with 1 meaning extremely low valence, 7 - extremely high, and 4 - neither high nor low.

SongID: 40
<audio controls>
  <source src="40.mp3" type="audio/mp3">
</audio>

SongID: 80
<audio controls>
  <source src="80.mp3" type="audio/mp3">
</audio>

SongID:155
<audio controls>
  <source src="155.mp3" type="audio/mp3">
</audio>

SongID: 202
<audio controls>
  <source src="202.mp3" type="audio/mp3">
</audio>

### Answers and Grading

First, the Mini-MSI results.

Each answer is worth a maximum of 7 points. For question 1, 3 and 4, the first option is worth 1 point, the second 2 points,... For question 2, the scores are reversed, with the first option being worth 7 points, the second 6 points,... Multiply your score by 4,5 to see what your score would’ve roughly been on the full test.

For the music we give the correct valence values. It is up to determine how far you were off or whether you were exactly correct.


| **Song 40**:
|   True valence: **4**
|   Average estimate: **5,5**
|   Times rated correct / Times tested: **1/10**
<br>

| **Song 80**:
|   True valence: **7**
|   Average estimate: **4,63**
|   Times rated correct / Times tested: **0/8**
<br>

| **Song 155**:
|   True valence: **2**
|   Average estimate: **2,78**
|   Times rated correct / Times tested: **4/9**
<br>

| **Song 202**:
|   True valence: **3**
|   Average estimate: **3,11**
|   Times rated correct / Times tested: **3/9**
<br>

On the following tab you can view the raw data of our participants, and you can see how well they did on the songs you listened to by searching for the songs' IDs (40, 80, 155, 202) in the searchbar



Background
===

Column {data-width=860}
---

### Introduction {data-height=810}

Nowadays, music is more present in our lives than ever before. According to the International Federation of the Phonographic Industry (2018), an average person spends around 18 hours a week listening to music. The majority of this time is spent on streaming platforms like Spotify, which have displayed a consistent growth in revenue over the past decade. The fact that music has become omnipresent in our lives raises the question whether this makes people better at identifying and assessing certain musical features.  In other words, does musical engagement affect how people perceive various features of music? If this would turn out to be true, this would indicate that the way we think about music, in fact, depends on our exposure to it.

Although this question has not been addressed directly in the academic literature, researchers have investigated how differences in musical engagement between people lead to differences in the perception of musical valence and arousal, with valence being defined as the positiveness of a track (Spotify, 2018), and arousal as the state of being alert or awake (Warriner et al., 1986). In their research, Olsen and colleagues (2014) conclude that such a difference exists: the variable musical engagement is a significant predictor of perceived musical arousal and valence. Interestingly, musical engagement is a factor of the more general concept of musical sophistication (Greenberg, Muellensiefen, Lamb & Rentfrow, 2015). However, the relationship between musical sophistication and perceived musical features has not been investigated by academics at the time of writing this report.

To fill this gap in the literature, we decided to investigate the following research question in our study: are musically sophisticated people better at estimating a musical piece’s valence? We specifically chose to scrutinize the relationship between musical sophistication and valence, as valence is a clearly defined, well studied emotional dimension in psychology (Frijda, 1986). To answer our question, we make use of surveys in which we ask our participants to estimate the valence of 25 randomly selected songs. We subsequently compare their outcomes not only between each other, but also to Spotify’s valence rating of the song: our point of reference used by the world’s biggest streaming platform (Spotify, 2018).




### Raw data 

```{r raw data}

raw_data <- read_excel("dataset_scatter_complete.xls")

raw_data <- raw_data[, -c(1, 2)]

DT::datatable(raw_data, options = list(bPaginate = FALSE), rownames = FALSE)

```

Column {.tabset}
---


### Materials

**Song pool**: the song pool contains 240 songs, with 20 songs per genre. The genres include: pop, rock, metal, electronic, dance, house, hip-hop, singer/songwriter, soundtrack, R&B, soul/blues and classical music. 

**Randomized playlist**: from the song pool, 25 songs are randomly selected for each participant to listen to and rate.

**Gold-MSI**: a questionnaire used to measure musical sophistication. Calculated as total number of points on the measure. Only the short version of the questionaire is used, i. e., only the items that map onto the general musical sophistication factor.

**Trial round**: prior to the main valence estimation task, participants are presented with two songs which they rate the valence of, and are then provided with the true valence of said songs. This is used to help the participants better conceptualize what valence is, and what the main task will look like.

**Valence rating**: a 7-point Likert scale, ranging from "Extremely low" (1) to "Extremely high" (7), used to estimate a song's valence.

**Familiarity measure**: a measured used to indicate whether the participant is familiar with the song that they are rating. This is used in later analysis to see if familiarity affects participant’s judgements of a song’s valence. Measured as 1 (familiar) or 0 (unfamiliar).

**R Shiny web app**: the web app is used to construct a randomized playlist from the song pool, administer the Gold-MSI, and collect valence and familiarity data.
         

### Song selection

In total, 242 songs were used in this experiment.

Two songs were the same for each participant, namely the trial round songs. These were selected to help participants better conceptualize musical valence. The first song had an extremely high valence value (7), whereas the second song was closer to the middle of the scale and had a slightly low value (3).

The selection process for the other 240 songs began with a breakdown of musical genres - we chose 12 genres based on a survey that was conducted using a sample of 19000 people between the age of 16-64, measuring the 12 most consumed genres of music in 18 countries. (IFPI, 2018). For each genre 20 songs were selected making use of the website ‘rateyourmusic.com’, where albums can be sorted by their average user-submitted rating, by genre. We sampled songs from these highly-rated albums, working under the assumption that well-regarded albums in a given genre are the most representative of said genre. Each sample of songs contained  only one song per artist, in a given genre.

During the selection process, we also took note of the valence of the sampled songs. We made sure that - per genre - no valence value was overrepresented or underrepresented, given what is usual for a specific genre; e. g., if a genre is generally characterized as having songs with higher valence, the song pool for said genre would be skewed towards, on average, higher values as well (compared to a song pool of a genre that is characterized as having lower valence songs).

Once the songs were selected, we trimmed each musical item's length. As a rule of thumb, we chose the 15 seconds from one minute of playback into the song. Some of the selected songs started with an intro, however, this did not count towards the minute.

### Procedure

Participants begin the experiment by completing the Gold-MSI to collect measures for musical sophistication. Participants are then  directed to the main experiment, where they are first  presented with a practice round. In the practice round, participants listen to two songs, and are then asked to rate each song’s valence. After each rating in the practice round, participants are presented with the song’s true valence as determined by Spotify. After this, participants proceed to the main task, which contains the randomized playlist. After listening to a song from the playlist, participants rate the valence using a 7-point Likert scale, and indicate whether they are familiar with the song.


Exploration {data-orientation=rows}
===

Sidebar {.sidebar data-width=300}
---
Having answered our main research question, we decided to engage in some exploratory research. On this page, we present 4 exploratory analyses and their appropriate interpretations. Of the things that we chose to analyze, we were particularly interested in: 

a) the density of valence estimation error per genre;

b) our main analysis dissected by genre; 

c) the relationship between Spotify's and our participants' valence ratings; 

d) how over/underestimated each genre was.


Row {.tabset data-height=600}
---
### Violin: Valence Estimation Error Density

```{r violin plots, fig.width=12}

data_long$signValDevFact <- as.factor(as.numeric(data_long$signValDev))

genre_statistics <-
  data_long %>%
  group_by(genre) %>%
  summarise(
    meanTruVal=mean(truVal),
    signValEstSD=sd(signValDev),
    corrs=cor(truVal, estVal),
    corrsMSI=cor(MSI_TOTAL, signValDev)
    )

data_long %>%
  ggplot(aes(x = genre, y = signValDev, color=genre)) +
  geom_violin() +
  geom_boxplot(width=0.1) +
  geom_hline(yintercept=0, linetype="dashed", color = "gray90") +
  scale_y_continuous(name = "Estimated Valence Error", limits = c(-6, 6)) +
  scale_x_discrete(name = element_blank()) +
  scale_color_brewer(palette="Paired") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "gray35"),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
        )

```

### Jitter: Musical Sophistication & Valence Estimation by Genre

```{r Lars graph, fig.width=8}

library(readxl)
dataset_scatter_complete <- read_excel("dataset_scatter_complete.xls")

dataset_scatter_complete$genre <-
  gsub('hiphop', 'Hip-Hop', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('classical', 'Classical', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('dance', 'Dance', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('electronic', 'Electronic', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('r&b', 'R&B', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('metal', 'Metal', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('rock', 'Rock', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('house', 'House', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('singersongwriter', 'Singer/Songwriter', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('pop', 'Pop', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('soundtracks', 'Soundtracks', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('soulblues', 'Soul/Blues', dataset_scatter_complete$genre)

# ##Scatter with all data
# ggplot(data = dataset_scatter_complete, aes( x= gold_msi, y=valence_error, color=genre)) +
#   geom_point() +
#   geom_smooth(method="lm")

##Small scatterplots per genre
scatterplot2 <-
  ggplot(data = dataset_scatter_complete,
         aes(x = gold_msi, y = valence_error, color = genre)) +
  geom_jitter(alpha=0.8) +
  geom_smooth(method="lm") +
  facet_wrap(~genre) +
  geom_text(data=genre_statistics,
          aes(label = paste("r = ", round(corrsMSI, 2), sep="")),
          x=95,
          y=-5.25,
          size=2.5,
          color="black") +
  scale_color_brewer(palette="Paired") +
  ggtitle("The relationship between valence estimation and musicality \nper genre") +
  coord_cartesian(xlim = c(30, 110), ylim = c(-6, 6), expand = TRUE) +
  scale_x_continuous(name = 'Gold-MSI Total Score', breaks = c(30, 70, 110)) +
  scale_y_continuous(name = 'Estimated Valence Error', breaks = c(-6, 0, 6)) +
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (12))
        )

scatterplot2
               

```

### Scatter: True Valence & Estimated Valence

```{r corr true and est, fig.width=8}

data_ggplot <- 
  read_excel("data_ggplot.xlsx")

data_ggplot$genre <-
  gsub('Singer-Songwriter', 'Singer/Songwriter', data_ggplot$genre)

data_ggplot2 <- 
  data_ggplot[!(is.na(data_ggplot$estimate_average) | data_ggplot$estimate_average==""), ]

scatter <-
  ggplot(data = data_ggplot2, aes( x= valence_value, y=estimate_average, color=genre)) +
  geom_point(alpha=0.8) +
  geom_smooth(method = "lm") +
  scale_color_brewer(palette="Paired") +
  facet_wrap(~genre) +
  geom_text(data=genre_statistics,
            aes(label = paste("r = ", round(corrs, 2), sep="")),
            x=6,
            y=1.5,
            size=2.5,
            color="black") +
  ggtitle("The relationship between Spotify's valence value and the average\nestimation of each song per genre") +
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (12))) + 
  coord_cartesian(xlim = c(1, 7), ylim = c(1, 7), expand = TRUE) +
  scale_x_continuous(name = 'True Valence', breaks = c(1, 4, 7)) +
  scale_y_continuous(name = 'Average Estimated Valence', breaks = c(1, 4, 7))

scatter

```

### Bar: Total Estimated Valence Error by Genre

```{r genres data, fig.width=6}

bar <- data_long %>%
  ggplot(aes(x = genre, y = signValDev, fill = signValDevFact)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  ggtitle("The total valence deviations per genre") +
  scale_x_discrete(name = "") +
  scale_y_continuous(name = 'Valence Deviation Total') +
  scale_fill_discrete(name="") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "gray35")
        )
ggplotly(bar)

write_csv(data_long, "data_long.csv")

```

Row {.tabset}
---
### Violin Interpretation
The violin graphs depict the density of estimated valence error responses per genre. This plot consists of two parts: one is the box plot, the other is the ‘violin’. From the box plot we can see the minimum and maximum responses, as well as the first quartile, median and third quantile. The thickness of the violin indicates the number of observations for a particular valence deviation.

From this graph we can deduct whether the valences in particular genres were systematically being over- or underestimated by participants. We see that for five genres this is not the case: Electronic, Hip-Hop, Pop, R&B and Singer/Songwriter all have a median of 0. For the other genres however, the medians range from -1 to +2. The most significant being Dance and House with an average of two points estimated above the true valence. For Soundtracks we see that the first quantile and median are equal to +1, but the third quantile is +4.

### Jitter Interpretation

In this jitter plot, the valence estimation error (i.e. the difference between the true valence and a participant’s estimation) is plotted against the Gold-MSI score and grouped per music genre. We made use of linear regression to investigate the potential relationship between the dependent and independent variable per genre. It is important to note that, as opposed to the plot on the main page, every dot in this graph represents one attempt of estimating the valence, not one participant. Because this means that we have multiple observations per MSI-value, the command geom_jitter() was used to avoid a straight line of points and make the graph more intuitive to interpret.

As becomes evident from looking at the graph, there seems to be no significant negative relationship between the valence estimation error and musical sophistication for any music genre like we hypothesized. Only the fitted lines for the genres pop, soul/blues and R&B show a very small negative slope. In fact, the genres hip-hop, rock and house even display a small positive relationship. Another interesting finding is that the genres house, dance and soundtracks seem to be systematically overvalued, as indicated by the fitted lines well above 0. This is in accordance with the findings from the violin plot and constitutes an interesting topic for further research.


### Scatter Interpretation

As can be seen, some of the genres have similar graphs, but overall, the graphs differ much from each other. A single graph in which all genres are included would not lead to a concise result, therefore we opted for genre-specific graphs. For most genres there is a positive trend, which is most accurately shown by the data for R&B and Singer-Songwriter. Another noteworthy result is that for Dance and House the lower valence values are overestimated on average, for Metal the higher valence values are underestimated on average. For Electronic, Hip-Hop and Rock most average estimates are around the central value, which either indicated a central tendency response pattern, or  that for these genres the positivity-grade is not clearly distinguishable.

What also can be seen is that for most genres, songs with extreme-valued valences, so 1 or 7, are not recognized by all participants the song was tested on. These results raise the question whether the valence values determined by Spotify are actually representative for the valence or positivity it evokes in people. The Soundtracks graph raises the same question, because Spotify determined all but one track to have valence value 1 or 2, but the on average judgement by the participants is very spread out.  


### Bar Interpretation

The bar charts depict the total valence error per genre. The bars are centered around 0, so it is possible to evidently see whether a genre is generally over- or underestimated. Each 'slice' in a bar represents an observation of a song in that genre. Moreover, The bars are colored by the estimated valence error, and the thickness of each 'slice' correspons to this error, e. g., if a song in the Rock genre was overvalued by 5, then the song's 'slice' is placed in the positive half of the column and has a thickness of 5 units. This means that if an observation was correct, i. e., had an error value of 0, then that slice is not depicted in the graph - this makes it easier to visually inspect whether a genre is generally over- or underestimated.

In accordance with the violin and jitter plots, it seems that songs in the Classical, Dance, House and Soundtracks genres were systematically overestimated. Furthermore, a valence error of 4-6 comprised an evidently larger part of the Classical, Dance, House and Soundtracks genres. When looking at both the scatterplot and bar chart, this suggests that Classical and Soundtracks are underestimated by Spotify, and that Dance and House are overestimated by people (this might be due to the energetic nature of songs in these genres).


Discussion
===

Column
---

### General conclusion {data-height=740}

The present study sought to investigate the relationship between musical sophistication and valence estimation error by comparing people's valence estimates against values provided by Spotify, while relating this to people's musical sophistication. Counter to the main hypohtesis, the results showed no negative relationship between musical sophistication and valence estimation error. This indicates that musical sophistication is not a predictor of accurate perception of musical valence. 

Subsequently, we conducted exploratory research to gain insights into relationships that we initially did not hypothesize. It seems that the only genre of music where our hypothesis holds is Soul/Blues; however, since we investigated 12 genres, it is plausible that the negative relationship observed is simply a statistical fluke. More interestingly, from multiple graphs, it can be seen that fragments in the Classical, House, Dance and Soundtracks genres were overestimated by our sample. It is plausible that this might be because the valences used by Spotify in these genres are undervalued, or because participants' ratings of these fragments were confounded by some other features of the tracks. Since these insights are of an exploratory nature though, they should not be taken as concrete conclusions; rather, they should be taken as advice for future research into the topic of musical sophistication and the identification of musical features.

### References and repository

Frijda, N. H. (1986). The emotions. Cambridge: Cambridge University Press.

[Greenberg, D. M., Müllensiefen, D., Lamb, M. E., & Rentfrow, P. J. (2015). Personality predicts musical sophistication. Journal of Research in Personality, 58, 154-158.](https://www.sciencedirect.com/science/article/pii/S0092656615000513?casa_token=Xyrsh3HI09cAAAAA:kkdBVw2rpGfibgnqM-DscQ1rXMID8ISKEGc79-WxNMu2Ny7O5NBpa7bDZpSqtuSmO8m5Jx2Q6Eo)

International Federation of the Phonographic Industry. (2018). IFPI Global Music Report 2019. Retrieved from: https://www.ifpi.org/news/IFPI-GLOBAL-MUSIC-REPORT-2018

[Müllensiefen, D., Gingras, B., Musil, J., & Stewart, L. (2014). The musicality of non-musicians: an index for assessing musical sophistication in the general population. PloS one.](https://psycnet.apa.org/record/2016-17512-001)

Spotify. (2018). Spotify Technology S.A. Announces Financial Results for First Quarter 2018. Retrieved from: https://investors.spotify.com/financials/press-release-details/2018/Spotify-Technology-SA-Announces-Financial-Results-for-First-Quarter-2018/default.aspx

Warriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior research methods, 45(4), 1191-1207.


https://github.com/knrds/everydayMusicListening


Column
---

### Limitations
**Sample size**

Our survey was completed by 42 participants. Because of the randomized and unique set of fragments that each participant listened to, this means that some of the fragments had relatively few observations, with 3 tracks having no observations entirely. While this does not influence the main analysis (as each participant's average score was used there), this does influence the relationship between songs' true valences provided and their estimated valences, as some of the datapoints in the graph plotting this relationship had nine observations, whereas some only had one.

**Survey limitations**

Musical fragments were only played once when participants took the survey. Because of this, if a participant failed to attend to a given fragment, they still had to rate its' valence with the limited insight they had. Furthermore, after receiving feedback from some participants, we realized that not all of the fragments were of an equal volume, and that some of the participants changed the volume multiple times during the survey - this also could have interfered with participants' attentiveness during the survey. This means that some of the valence ratings could have been different, and maybe even more accurate, if participants had had the capability to replay songs.

**Sample representativeness**

Our sample mainly consisted of undergraduates studying in the Netherlands. This likely only forms a particular subset of the sample that Spotify utilizes for computing the various track features it offers, including valence. Therefore, if we had utilized a sample represenatitive of the population that uses Spotify (and not a subset of it), it's plausible that the valence estimates we observed would be more in line with those provided by Spotify.


### Future research

The research we conducted raises several questions which could be addressed in further academic research. First of all, it would be of particular interest to verify whether our finding that musically sophisticated individuals are no better at estimating the valence of a song holds for other music features as well. It would also be interesting to find out how our conclusion changes if more objectively measurable features of music were used, such as key, mode (major vs. minor) or tempo. If the results of such research would correspond to our findings, this would imply more everyday music exposure does not make us better at identifying music characteristics in any way.

Second, the fact that the valences of songs in the Classical, House, Dance and Soundtracks genres were systematically overvalued in our sample (if we are willing to accept the assumption that Spotify’s valence values can be regarded as true) makes one wonder about the possible causes of this empirical observation. For instance, is it difficult to assess the valence of these genres because these songs often contain no or very little vocals that convey a message? Would people rate the valences of songs differently depending on whether they contain vocals? Or, as House and Dance are genres with a relatively high tempo, might the overvaluation be linked to the tempo of the songs? These are all questions that remain to be answered yet.






