---
title: "Musical Sophistication and Valence Estimation"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    theme: cosmo
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(plotly)
library(Cairo)
library(DT)
library(reshape2)
library(data.table)
library(varhandle)
library(readxl)
```


```{r survey data}
#data cleaning 

data_wide <-
  read_csv('survey upd1.csv', TRUE)

#deleting irrelevant columns
data_wide <- data_wide[, -c(1:8, 24, 28:29)]

#COLUMNS 1:18 ARE MSI RESULTS
#COLUMN 19 IS SONG ORDER

#deleting all instances of 'MSI' in front of a value
data_wide <- data.frame(lapply(data_wide, function(x){
  gsub("MSI", "", x)
}))

#renaming the MSI columns
names(data_wide)[1:18] = paste0("MSI", 1:18)

#renaming the valence columns
names(data_wide)[c(20:69)[seq(1, 50, 2)]] = paste0("song", 1:25, "_valence")

#renaming the familiarity columns
names(data_wide)[c(20:69)[seq(2, 50, 2)]] = paste0("song", 1:25, "_familiarity")

#splitting the columns from "songID_valenceEst" to column "songID" and column "valenceEst"
setDT(data_wide)
cols <- grep("song", names(data_wide), value = TRUE)
for (i in cols) {
  temp <- tstrsplit(data_wide[[i]], "_")
  set(data_wide, j = sprintf("%s_%d", i, seq_along(temp)), value = temp)
  set(data_wide, j = i, value = NULL)
}

#deleting duplicate-value columns (familiarity_1)
data_wide <- data_wide[, -c(20:119)[seq(3, 100, 4)]]

names(data_wide) <- gsub("valence_1", "ID", names(data_wide))
names(data_wide) <- gsub("valence_2", "estVal", names(data_wide))
names(data_wide) <- gsub("familiarity_2", "familiar", names(data_wide))

data_wide <- unfactor(data_wide)

part_data_wide <-
  read_csv2("participants_results.csv")

#deleting irrelevant columns
part_data_wide <- part_data_wide[, -1]

#renaming the deviation columns
names(part_data_wide)[c(4:6)] = paste0("dev", 1:3)
names(part_data_wide)[1] = "MSI_total"
names(part_data_wide)[2] = "ValOff_total"

data_wide$MSI1 <- part_data_wide$MSI_total
data_wide <- data_wide[, -c(2:18)]
names(data_wide)[1] = "MSI_TOTAL"

```



```{r lengthening}

data_long <- melt(setDT(data_wide), id=1L,
                  measure = patterns("ID$", "estVal$", "familiar$"),
                  value.name = c("song_ID", "estVal", "familiarity"))

data_long$song_ID <- as.numeric(as.character(data_long$song_ID))
data_long$estVal <- as.numeric(as.character(data_long$estVal))
data_long$variable <- as.numeric(as.character(data_long$variable))

data_long$genre <-
  ifelse(data_long$song_ID < 21, "Pop",
         ifelse(data_long$song_ID < 41, "Rock",
                ifelse(data_long$song_ID < 61, "Dance",
                       ifelse(data_long$song_ID < 81, "Electronic",
                              ifelse(data_long$song_ID < 101, "House",
                                     ifelse(data_long$song_ID < 121, "Soundtracks",
                                            ifelse(data_long$song_ID < 141, "Hip-Hop",
                                                   ifelse(data_long$song_ID < 161, "Singer/Songwriter",
                                                          ifelse(data_long$song_ID < 181, "Classical",
                                                                 ifelse(data_long$song_ID < 201, "R&B",
                                                                        ifelse(data_long$song_ID < 221, "Soul/Blues", "Metal")
                                                                        )
                                                                 )
                                                          )
                                                   )
                                            )
                                     )
                              )
                       )
                )
         )
  
data_long <- data_long[order(data_long$song_ID)]
```

```{r valencing}
valences_data <-
  read_csv("valences.csv")

for(i in 1:nrow(data_long)) {
    data_long$truVal <- valences_data$valence[data_long$song_ID]
}

data_long$signValDev <- data_long$estVal - data_long$truVal

```


Main Findings {data-orientation=rows}
===

Sidebar {.sidebar data-width=350}
---

**Study abstract**

The present study investigated the relationship between musical sophistication and musical valence estimation. This was done by having participants listen to a random selection of songs, representative of 12 genres, and collecting their valence estimations of each song via an R Shiny app. Musical sophistication was measured using the Gold-MSI questionnaire, and musical valence estimation was measured using a 7-point Likert scale. The results showed no negative correlation between scores on the Gold-MSI and Likert scale rating. Based on the above, we conclude that there is no relationship between musical sophistication and musical valence estimation.

**Explanation of terms**

**Valence** - the intrinsic positiveness that is associated, in this case, with a musical stimulus. High valences indicates positive emotional states such as happiness/cheerfulness, whereas low valence indicates negative emotional dimensions such as sadness/gloominess.

**Musical sophistication** - the musical attitudes, skills and expertise of an individual.

**Hypothesis**

There is a negative relationship between musical sophistication and musical valence estimation error.

**Analysis & Results**

Analysis was conducted using R's built-in cor.test() function, running a Pearson's product-moment correlation.

Correlation: Gold-MSI & Valence Estimation Error 

<font style = "font-family: times, serif; font-size:16pt;"> (*r* = 0.13, *p* = 0.79) </font>

**Scatterplot interpretation**

Higher scores on the x-axis (Gold-MSI Score) indicate higher musical sophistication.

Higher scores on the y-axis (Average Valence Estimation Error) indicate, on average, higher error (and in turn, lower accuracy) when estimating a musical fragment's valence.

Column
---

### Scatterplot: Musical Sophistication and Valence Estimation Error {data-height=800}

```{r mainplot, fig.width = 8}

mainplot_corr <- cor.test(part_data_wide$MSI_total, part_data_wide$ValOff_total, "less")

options(repr.plot.width=800, repr.plot.height=500)
mainplot <- part_data_wide %>%
  ggplot(aes(x = MSI_total, y = ValOff_total/25)) +
  geom_point() +
  geom_smooth(method = "lm") +
  annotate(x = 100,
           y = 37/25,
           label=paste("r = ",
                       round(cor(part_data_wide$MSI_total, part_data_wide$ValOff_total),2)
                       ),
         geom="text", size=5) +
  scale_x_continuous(name = 'Gold-MSI Score') +
  scale_y_continuous(name = 'Average Valence Estimation Error') +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "gray")
        )

ggplotly(mainplot)
```


Column {.tabset}
---

### Interactive Experiment: Intro

We challenge you to beat our participants! In this experiment demo you get to experience what our research is about in an interactive way. In the following tabs we have included short versions of the different parts in our survey.
First, you get to make a mini Gold-MSI test. The result places you on the musical sophistication scale. The higher your score, the more knowledge and experience you have with music.
The second part consists of a few songs that we used in the survey. It is up to you to determine the valence value of each song to the best of your ability! For both the Gold-MSI test and the song estimation test, the results are on the subsequent tab.
After you have determined your scores, you can see in the graph where you would have approximately been in our sample and if you have surpassed the participants. How to grade yourself is further explained on the answer tabs. Good luck!

### Mini-MSI

### Musical Fragments

### Answers and Grading


Background
===

Column {data-width=860}
---

### Introduction {data-height=810}

Nowadays, music is more present in our lives than ever before. According to the International Federation of the Phonographic Society (2018), an average person spends around 18 hours a week listening to music. The majority of this time is spent on streaming platforms like Spotify, which have displayed a consistent growth in revenue over the past decade. The fact that music has become omnipresent in our lives raises the question whether this makes people better at identifying and assessing certain musical features. In other words, is there a difference between how normal and musically sophisticated individuals perceive features of music? If this would turn out to be true, this would indicate that the way we think about music, in fact, depends on our exposure to it.

Although this question has not been addressed directly in the academic literature, researchers have investigated how differences in musical engagement between people lead to differences in the perception of musical valence and arousal, with valence being defined as the positiveness of a track (Frijda, 1986), and arousal as the state of being alert or awake (Warriner et al., 1986). In their research, Olsen and colleagues (2014) conclude that such a difference exists: the variable musical engagement is a significant predictor of perceived musical arousal and valence. However, the relationship between musical sophistication and perceived musical features has not been investigated by academics at the time of writing this paper.

To fill this gap in the literature, we decided to investigate the following research question in our study: are musically sophisticated people better at estimating a musical pieceâ€™s valence? We specifically chose to scrutinize the relationship between musical sophistication and valence, as valence is a clearly defined, well studied emotional dimension in psychology. To answer our question, we make use of surveys in which we ask our participants to estimate the valence of 25 randomly selected songs. We subsequently compare their outcomes not only between each other, but also to Spotifyâ€™s valence rating of the song: our point of reference used by the worldâ€™s biggest streaming platform (Spotify, 2018).




### Raw data 

```{r raw data}

DT::datatable(data_wide, options = list(
  bPaginate = FALSE
))

```

Column {.tabset}
---


### Materials

**Song pool**: the song pool contains 240 songs, with 20 songs per genre. The genres include: pop, rock, metal, electronic, dance, house, hip-hop, singer/songwriter, soundtrack, R&B, soul/blues and classical music. 

**Randomized playlist**: from the song pool, 25 songs are randomly selected for each participant to listen to and rate.

**Gold-MSI**: a questionnaire used to measure musical sophistication. Calculated as total number of points on the measure. Only the short version of the questionaire is used, i. e., only the items that map onto the general musical sophistication factor.

**Trial round**: prior to the main valence estimation task, participants are presented with two songs which they rate the valence of, and are then provided with the true valence of said songs. This is used to help the participants better conceptualize what valence is, and what the main task will look like.

**Valence rating**: a 7-point Likert scale, ranging from "Extremely low" (1) to "Extremely high" (7), used to estimate a song's valence.

**Familiarity measure**: a measured used to indicate whether the participant is familiar with the song that they are rating. This is used in later analysis to see if familiarity affects participantâ€™s judgements of a songâ€™s valence. Measured as 1 (familiar) or 0 (unfamiliar).

**R Shiny web app**: the web app is used to construct a randomized playlist from the song pool, administer the Gold-MSI, and collect valence and familiarity data.
         

### Song selection

In total, 242 songs were used in this experiment.

Two songs were the same for each participant, namely the trial round songs. These were selected to help participants better conceptualize musical valence. The first song had an extremely high valence value (7), whereas the second song was closer to the middle of the scale and had a slightly low value (3).

The selection process for the other 240 songs began with a breakdown of musical genres - we chose 12 genres based on a survey that was conducted using a sample of 19000 people between the age of 16-64, measuring the 12 most consumed genres of music in the U.S. (REFERENCE). For each genre 20 songs were selected making use of the website â€˜rateyourmusic.comâ€™, where albums can be sorted by their average user-submitted rating, by genre. We sampled songs from these highly-rated albums, working under the assumption that well-regarded albums in a given genre are the most representative of said genre. Each sample of songs contained  only one song per artist, in a given genre.

During the selection process, we also took note of the valence of the sampled songs. We made sure that - per genre - no valence value was overrepresented or underrepresented, given what is usual for a specific genre; e. g., if a genre is generally characterized as having songs with higher valence, the song pool for said genre would be skewed towards, on average, higher values as well (compared to a song pool of a genre that is characterized as having lower valence songs).

Once the songs were selected, we trimmed each musical item's length. As a rule of thumb, we chose the 15 seconds from one minute of playback into the song. Some of the selected songs started with an intro, however, this did not count towards the minute.

### Procedure

Participants begin the experiment by completing the Gold-MSI to collect measures for musical sophistication. Participants are then  directed to the main experiment, where they are first  presented with a practice round. In the practice round, participants listen to two songs, and are then asked to rate each songâ€™s valence. After each rating in the practice round, participants are presented with the songâ€™s true valence as determined by Spotify. After this, participants proceed to the main task, which contains the randomized playlist. After listening to a song from the playlist, participants rate the valence using a 7-point Likert scale, and indicate whether they are familiar with the song.


Exploration {data-orientation=rows}
===

Sidebar {.sidebar data-width=300}
---
Text about how this is exploratory research, and how **hard** conclusions cannot be drawn from these graphs, but how they nevertheless can guide future research.

Plus, some info on the different types of graphs


Row {.tabset data-height=600}
---
### Violin: Valence Estimation Deviation Density

```{r violin plots, fig.width=12}

data_long$signValDevFact <- as.factor(as.numeric(data_long$signValDev))

genre_statistics <- data_long %>%
  group_by(genre) %>%
  summarise(
    meanTruVal=mean(truVal),
    signValEstSD=sd(signValDev)
    )

data_long %>%
  ggplot(aes(x = genre, y = signValDev, color=genre)) +
  geom_violin() +
  geom_boxplot(width=0.1) +
  geom_hline(yintercept=0, linetype="dashed", color = "gray90") +
  scale_y_continuous(name = "Valence Estimate Deviation", limits = c(-6, 6)) +
  scale_x_discrete(name = element_blank()) +
  scale_color_brewer(palette="Paired") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "gray35"),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
        )

```

### Jitter: Musical Sophistication & Valence Estimation by Genre

```{r Lars graph, fig.width=8}

library(readxl)
dataset_scatter_complete <- read_excel("dataset_scatter_complete.xls")

dataset_scatter_complete$genre <-
  gsub('hiphop', 'Hip-Hop', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('classical', 'Classical', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('dance', 'Dance', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('electronic', 'Electronic', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('r&b', 'R&B', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('metal', 'Metal', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('rock', 'Rock', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('house', 'House', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('singersongwriter', 'Singer/Songwriter', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('pop', 'Pop', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('soundtracks', 'Soundtracks', dataset_scatter_complete$genre)
dataset_scatter_complete$genre <-
  gsub('soulblues', 'Soul/Blues', dataset_scatter_complete$genre)

# ##Scatter with all data
# ggplot(data = dataset_scatter_complete, aes( x= gold_msi, y=valence_error, color=genre)) +
#   geom_point() +
#   geom_smooth(method="lm")

##Small scatterplots per genre
scatterplot2 <-
  ggplot(data = dataset_scatter_complete,
         aes(x = gold_msi, y = valence_error, color = genre)) +
  geom_jitter(alpha=0.5) +
  geom_smooth(method="lm") +
  facet_wrap(~genre) +
  scale_color_brewer(palette="Paired") +
  ggtitle("The relationship between valence estimation and musicality \nper genre") +
  coord_cartesian(xlim = c(30, 110), ylim = c(-6, 6), expand = TRUE) +
  scale_x_continuous(name = 'Gold-MSI Total Score', breaks = c(30, 70, 110)) +
  scale_y_continuous(name = 'Estimated Valence Error', breaks = c(-6, 0, 6)) +
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (12))
        )

scatterplot2
               

```

### Scatter: True Valence & Estimated Valence

```{r corr true and est, fig.width=8}

data_ggplot <- 
  read_excel("data_ggplot.xlsx")

scatterplot1 <-
  ggplot(data = data_ggplot, aes( x= valence_value, y=estimate_average, color=genre)) +
  geom_point(alpha=0.8) +
  geom_smooth(method = "lm") +
  facet_wrap(~genre) +
  ggtitle("The relationship between Spotify's valence value and the average\nestimation of each song per genre") +
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (12))) + 
  coord_cartesian(xlim = c(1, 7), ylim = c(1, 7), expand = TRUE) +
  scale_x_continuous(name = 'True Valence', breaks = c(1, 4, 7)) +
  scale_y_continuous(name = 'Average Estimated Valence', breaks = c(1, 4, 7)) +
  scale_color_brewer(palette="Paired")

scatterplot1

```

### Bar: Average Valence Estimation Deviation

```{r genres data, fig.width=10}

genres_data <- read_csv2("genres_results.csv")
  

data_long %>%
  ggplot(aes(x = genre, y = signValDev, fill = signValDevFact)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_x_discrete(name = element_blank()) +
  scale_y_continuous(name = 'Valence Deviation Count') +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "gray35")
        )

```

Row {.tabset}
---
### Violin Interpretation
The violin graph plots the 12 genres against the average valence estimation deviation (i.e. the average of difference between what participants estimated and Spotifyâ€™s true valence). This plot consists of two parts: one is the box plot, the other is the â€˜violinâ€™. From the box plot we can see the minimum and maximum values, as well as the first quantile, median and third quantile. The thickness of the violin indicates the number of observations for a particular valence deviation.

From this graph we can deduct whether the valences in particular genres were systematically being over- or underestimated by participants. We see that for five genres this is not the case: Electronic, Hip-Hop, Pop, R&B and Singer/Songwriter all have a median of 0. For the other genres however, the medians range from -1 to +2. The most significant being Dance and House with an average of two points estimated above the true valence. For Soundtracks we see that the first quantile and median are equal to +1, but the third quantile is +4.

### Jitter Interpretation

### Scatter Interpretation

As you can see, some genres have similar graphs, but overall the graphs differ much from each other. A single graph in which all genres are included would not lead to a concise result. However, the graphs do show pretty interesting genre specific results. For most genres there is a positive trend, which is most accurately shown by the data for R&B and Singer-Songwriter. Another noteworthy result is that for Dance and House the lower valence values are overestimated on average, for Metal the higher valence values are underestimated on average. For Electronic, Hip-Hop and Rock most average estimates are around the central value, which indicates that for these genres the positivity-grade is not clearly distinguishable.

What also can be seen is that for most genres, songs with extreme-valued valences, so 1 or 7, are not recognized by all participants the song was tested on. These results raise the question whether the valence values determined by Spotify are actually representative for the valence or positivity it evokes in people. The Soundtracks graph raises the same question, because Spotify determined all but one track to have valence value 1 or 2, but the on average judgement by the participants is very spread out.  


### Bar Interpretation


Discussion
===

Column
---

### General conclusion 

The present study sought to investigate the relationship between musical sophistication and valence estimation by comparing people's valence estimates against values provided by Spotify. Counter to the main hypohtesis, the results showed a weak positive relationship between musical sophistication and valence estimation inaccuracy. This indicates that musical sophistication is not a predictor of accurate perception of musical valence, and that musical sophistication might even interfere with valence perception, though this should be taken with a grain (or nugget) of salt. MISSING INFO: GENRE BREAKDOWN. MISSING INFO: CORRELATION WITH SPOTIFY'S VALUES AND DISCUSSION OF THESE RESULTS.


Column
---

### Limitations
Discuss limitations


### Future research

Discuss future research into the area

Appendix
===

Column
---

### References

Frijda, N. H. (1986). The emotions. Cambridge: Cambridge University Press.

International Federation of the Phonographic Society. (2018). IFPI Global Music Report 2019. Retrieved from: https://www.ifpi.org/news/IFPI-GLOBAL-MUSIC-REPORT-2019

Spotify. (2018). Spotify Technology S.A. Announces Financial Results for First Quarter 2018. Retrieved from: https://investors.spotify.com/financials/press-release-details/2018/Spotify-Technology-SA-Announces-Financial-Results-for-First-Quarter-2018/default.aspx

Warriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior research methods, 45(4), 1191-1207.

Column
---

### Downloadables

Links to our GitHub repositories, and to downloadables of the data we used

